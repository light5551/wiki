![[Pasted image 20231028234301.png]]
## Постановка задачи

Теперь попробуем формализовать всю эту концепцию и познакомиться с местной терминологией. Задача обучения с подкреплением задаётся **Марковским Процессом Принятия Решений** (**Markov Decision Process** или сокращённо **MDP**) это четвёрка (S,A,P,r), где:

- S — **пространство состояний** (state space), множество состояний, в которых в каждый момент времени может находится среда.
- A — **пространство действий** (action space), множество вариантов, из которых нужно производить выбор на каждом шаге своего взаимодействия со средой.
- P — **функция переходов** (transition function), которая задаёт изменение среды после того, как в состоянии s∈S было выбрано действие a∈A. В общем случае функция переходов может быть стохастична, и тогда такая функция переходов моделируется распределением p(s′∣s,a): с какой вероятностью в какое состояние перейдёт среда после выбора действия a в состоянии s.
- r:S×A→R — **функция награды** (reward function), выдающая скалярную величину за выбор действия a в состоянии s. Это наш «обучающий сигнал».
## Что такое Q-функция? 


## Что такое V-функция? 

